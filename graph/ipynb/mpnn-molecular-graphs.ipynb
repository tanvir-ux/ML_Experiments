{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Message-passing neural network for molecular property prediction\n",
    "Author: [akensert](http://github.com/akensert)\n",
    "Date created: 2021/08/16\n",
    "Last modified: 2021/08/16\n",
    "Description: Implementation of an MPNN to predict blood-brain barrier permeability.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, we will implement a type of graph neural network (GNN) known as\n",
    "_ message passing neural network_ (MPNN) to predict graph properties. Specifically, we will\n",
    "implement an MPNN to predict a molecular property known as\n",
    "_blood-brain barrier permeability_ (BBBP).\n",
    "\n",
    "Motivation: as molecules are naturally represented as an undirected graph `G = (V, E)`,\n",
    "where `V` is a set or vertices (nodes; atoms) and `E` a set of edges (bonds), GNNs (such\n",
    "as MPNN) are proving to be a useful method for predicting molecular properties.\n",
    "\n",
    "Until now, more traditional methods, such as random forests, support vector machines, etc.,\n",
    "have been commonly used to predict molecular properties. In contrast to GNNs, these\n",
    "traditional approaches often operate on precomputed molecular features such as\n",
    "molecular weight, polarity, charge, number of carbon atoms, etc. Although these\n",
    "molecular features prove to be good predictors for various molecular properties, it is\n",
    "hypothesized that operating on these more \"raw\", \"low-level\", features could prove even\n",
    "better.\n",
    "\n",
    "### References\n",
    "\n",
    "In recent years, a lot of effort has been put into developing neural networks for\n",
    "graph data, including molecular graphs. For a summary of graph neural networks, see e.g.,\n",
    "[A Comprehensive Survey on Graph Neural Networks](https://arxiv.org/abs/1901.00596) and\n",
    "[Graph Neural Networks: A Review of Methods and Applications](https://arxiv.org/abs/1812.08434);\n",
    "and for further reading on the specific\n",
    "graph neural network implemented in this tutorial see\n",
    "[Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212) and\n",
    "[DeepChem's MPNNModel](https://deepchem.readthedocs.io/en/latest/api_reference/models.html#mpnnmodel).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Setup\n",
    "\n",
    "### Install RDKit and other dependencies\n",
    "\n",
    "(Text below taken from\n",
    "[this tutorial](https://keras.io/examples/generative/wgan-graphs/)).\n",
    "\n",
    "[RDKit](https://www.rdkit.org/) is a collection of cheminformatics and machine-learning\n",
    "software written in C++ and Python. In this tutorial, RDKit is used to conveniently and\n",
    "efficiently transform\n",
    "[SMILES](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) to\n",
    "molecule objects, and then from those obtain sets of atoms and bonds.\n",
    "\n",
    "SMILES expresses the structure of a given molecule in the form of an ASCII string.\n",
    "The SMILES string is a compact encoding which, for smaller molecules, is relatively\n",
    "human-readable. Encoding molecules as a string both alleviates and facilitates database\n",
    "and/or web searching of a given molecule. RDKit uses algorithms to\n",
    "accurately transform a given SMILES to a molecule object, which can then\n",
    "be used to compute a great number of molecular properties/features.\n",
    "\n",
    "Notice, RDKit is commonly installed via [Conda](https://www.rdkit.org/docs/Install.html).\n",
    "However, thanks to\n",
    "[rdkit_platform_wheels](https://github.com/kuelumbus/rdkit_platform_wheels), rdkit\n",
    "can now (for the sake of this tutorial) be installed easily via pip, as follows:\n",
    "\n",
    "```\n",
    "pip -q install rdkit-pypi\n",
    "```\n",
    "\n",
    "And for easy and efficient reading of csv files and visualization, the below needs to be\n",
    "installed:\n",
    "\n",
    "```\n",
    "pip -q install pandas\n",
    "pip -q install Pillow\n",
    "pip -q install matplotlib\n",
    "pip -q install pydot\n",
    "sudo apt-get -qq install graphviz\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Import packages\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem.Draw import MolsToGridImage\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RDLogger.DisableLog(\"rdApp.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Dataset\n",
    "\n",
    "Information about the dataset can be found in\n",
    "[A Bayesian Approach to in Silico Blood-Brain Barrier Penetration Modeling](https://pubs.acs.org/doi/10.1021/ci300124c)\n",
    "and [MoleculeNet: A Benchmark for Molecular Machine Learning](https://arxiv.org/abs/1703.00564).\n",
    "The dataset will be downloaded from [MoleculeNet.ai](http://moleculenet.ai/datasets-1).\n",
    "\n",
    "### About\n",
    "\n",
    "The dataset contains **2,050** molecules. Each molecule come with a **name**, **label**\n",
    "and **SMILES** string.\n",
    "\n",
    "The blood-brain barrier (BBB) is a membrane separating the blood from the brain\n",
    "extracellular fluid, hence blocking out most drugs (molecules) from reaching\n",
    "the brain. Because of this, the BBBP has been important to study for the development of\n",
    "new drugs that aim to target the central nervous system. The labels for this\n",
    "data set are binary (1 or 0) and indicate the permeability of the molecules.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = keras.utils.get_file(\n",
    "    \"BBBP.csv\", \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/BBBP.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path, usecols=[1, 2, 3])\n",
    "df.iloc[96:104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Define features\n",
    "\n",
    "To encode features for atoms and bonds (which we will need later),\n",
    "we'll define two classes: `AtomFeaturizer` and `BondFeatuzier` respectively.\n",
    "\n",
    "To reduce the lines of code, i.e., to keep this tutorial short and concise,\n",
    "only about a handful of (atom and bond) features will be considered: \\[atom features\\]\n",
    "[symbol (element)](https://en.wikipedia.org/wiki/Chemical_element),\n",
    "[number of valence electrons](https://en.wikipedia.org/wiki/Valence_electron),\n",
    "[number of hydrogen bonds](https://en.wikipedia.org/wiki/Hydrogen),\n",
    "[orbital hybridization](https://en.wikipedia.org/wiki/Orbital_hybridisation),\n",
    "\\[bond features\\]\n",
    "[(covalent) bond type](https://en.wikipedia.org/wiki/Covalent_bond), and\n",
    "[conjugation](https://en.wikipedia.org/wiki/Conjugated_system).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Featurizer:\n",
    "    def __init__(self, allowable_sets):\n",
    "        self.dim = 0\n",
    "        self.features_mapping = {}\n",
    "        for k, s in allowable_sets.items():\n",
    "            s = sorted(list(s))\n",
    "            self.features_mapping[k] = dict(zip(s, range(self.dim, len(s) + self.dim)))\n",
    "            self.dim += len(s)\n",
    "\n",
    "    def encode(self, inputs):\n",
    "        output = np.zeros((self.dim,))\n",
    "        for name_feature, feature_mapping in self.features_mapping.items():\n",
    "            feature = getattr(self, name_feature)(inputs)\n",
    "            if feature not in feature_mapping:\n",
    "                continue\n",
    "            output[feature_mapping[feature]] = 1.0\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomFeaturizer(Featurizer):\n",
    "    def __init__(self, allowable_sets):\n",
    "        super().__init__(allowable_sets)\n",
    "\n",
    "    def symbol(self, atom):\n",
    "        return atom.GetSymbol()\n",
    "\n",
    "    def n_valence(self, atom):\n",
    "        return atom.GetTotalValence()\n",
    "\n",
    "    def n_hydrogens(self, atom):\n",
    "        return atom.GetTotalNumHs()\n",
    "\n",
    "    def hybridization(self, atom):\n",
    "        return atom.GetHybridization().name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BondFeaturizer(Featurizer):\n",
    "    def __init__(self, allowable_sets):\n",
    "        super().__init__(allowable_sets)\n",
    "        self.dim += 1\n",
    "\n",
    "    def encode(self, bond):\n",
    "        output = np.zeros((self.dim,))\n",
    "        if bond is None:\n",
    "            output[-1] = 1.0\n",
    "            return output\n",
    "        output = super().encode(bond)\n",
    "        return output\n",
    "\n",
    "    def bond_type(self, bond):\n",
    "        return bond.GetBondType().name.lower()\n",
    "\n",
    "    def conjugated(self, bond):\n",
    "        return bond.GetIsConjugated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_featurizer = AtomFeaturizer(\n",
    "    allowable_sets={\n",
    "        \"symbol\": {\"B\", \"Br\", \"C\", \"Ca\", \"Cl\", \"F\", \"H\", \"I\", \"N\", \"Na\", \"O\", \"P\", \"S\"},\n",
    "        \"n_valence\": {0, 1, 2, 3, 4, 5, 6},\n",
    "        \"n_hydrogens\": {0, 1, 2, 3, 4},\n",
    "        \"hybridization\": {\"s\", \"sp\", \"sp2\", \"sp3\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "bond_featurizer = BondFeaturizer(\n",
    "    allowable_sets={\n",
    "        \"bond_type\": {\"single\", \"double\", \"triple\", \"aromatic\"},\n",
    "        \"conjugated\": {True, False},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Generate graphs\n",
    "\n",
    "Before we can generate complete graphs from SMILES, we need to implement the following functions:\n",
    "\n",
    "1. `molecule_from_smiles`, which takes as input a SMILES and returns a molecule object.\n",
    "This is all handled by RDKit.\n",
    "\n",
    "2. `graph_from_molecule`, which takes as input a molecule object and returns a graph,\n",
    "represented as a three-tuple (atom_features, bond_features, pair_indices). For this we\n",
    "will make use of the classes defined previously.\n",
    "\n",
    "Finally, we can now implement the function `graphs_from_smiles`, which applies function (1)\n",
    "and subsequently (2) on all SMILES of the training, validation and test datasets.\n",
    "\n",
    "Notice: although scaffold splitting is recommended for this data set (see\n",
    "[here](https://arxiv.org/abs/1703.00564)), for simplicity, simple random splittings were\n",
    "performed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def molecule_from_smiles(smiles):\n",
    "    # MolFromSmiles(m, sanitize=True) should be equivalent to\n",
    "    # MolFromSmiles(m, sanitize=False) -> SanitizeMol(m) -> AssignStereochemistry(m, ...)\n",
    "    molecule = Chem.MolFromSmiles(smiles, sanitize=False)\n",
    "\n",
    "    # If sanitization is unsuccessful, catch the error, and try again without\n",
    "    # the sanitization step that caused the error\n",
    "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
    "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "        Chem.SanitizeMol(molecule, sanitizeOps=Chem.SanitizeFlags.SANITIZE_ALL ^ flag)\n",
    "\n",
    "    Chem.AssignStereochemistry(molecule, cleanIt=True, force=True)\n",
    "    return molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_from_molecule(molecule):\n",
    "    # Initialize graph\n",
    "    atom_features = []\n",
    "    bond_features = []\n",
    "    pair_indices = []\n",
    "\n",
    "    for atom in molecule.GetAtoms():\n",
    "        atom_features.append(atom_featurizer.encode(atom))\n",
    "\n",
    "        # Add self-loop. Notice, this also helps against some edge cases where the\n",
    "        # last node has no edges. Alternatively, if no self-loops are used, for these\n",
    "        # edge cases, zero-padding on the output of the edge network is needed.\n",
    "        pair_indices.append([atom.GetIdx(), atom.GetIdx()])\n",
    "        bond_features.append(bond_featurizer.encode(None))\n",
    "\n",
    "        atom_neighbors = atom.GetNeighbors()\n",
    "\n",
    "        for neighbor in atom_neighbors:\n",
    "            bond = molecule.GetBondBetweenAtoms(atom.GetIdx(), neighbor.GetIdx())\n",
    "            pair_indices.append([atom.GetIdx(), neighbor.GetIdx()])\n",
    "            bond_features.append(bond_featurizer.encode(bond))\n",
    "\n",
    "    return np.array(atom_features), np.array(bond_features), np.array(pair_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphs_from_smiles(smiles_list):\n",
    "    # Initialize graphs\n",
    "    atom_features_list = []\n",
    "    bond_features_list = []\n",
    "    pair_indices_list = []\n",
    "\n",
    "    for smiles in smiles_list:\n",
    "        molecule = molecule_from_smiles(smiles)\n",
    "        atom_features, bond_features, pair_indices = graph_from_molecule(molecule)\n",
    "\n",
    "        atom_features_list.append(atom_features)\n",
    "        bond_features_list.append(bond_features)\n",
    "        pair_indices_list.append(pair_indices)\n",
    "\n",
    "    # Convert lists to ragged tensors for tf.data.Dataset later on\n",
    "    return (\n",
    "        tf.ragged.constant(atom_features_list, dtype=tf.float32),\n",
    "        tf.ragged.constant(bond_features_list, dtype=tf.float32),\n",
    "        tf.ragged.constant(pair_indices_list, dtype=tf.int64),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle array of indices ranging from 0 to 2049\n",
    "permuted_indices = np.random.permutation(np.arange(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set: 80 % of data\n",
    "train_index = permuted_indices[: int(df.shape[0] * 0.8)]\n",
    "x_train = graphs_from_smiles(df.iloc[train_index].smiles)\n",
    "y_train = df.iloc[train_index].p_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid set: 19 % of data\n",
    "valid_index = permuted_indices[int(df.shape[0] * 0.8) : int(df.shape[0] * 0.99)]\n",
    "x_valid = graphs_from_smiles(df.iloc[valid_index].smiles)\n",
    "y_valid = df.iloc[valid_index].p_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set: 1 % of data\n",
    "test_index = permuted_indices[int(df.shape[0] * 0.99) :]\n",
    "x_test = graphs_from_smiles(df.iloc[test_index].smiles)\n",
    "y_test = df.iloc[test_index].p_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Test the functions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Name:\\t{df.name[100]}\\nSMILES:\\t{df.smiles[100]}\\nBBBP:\\t{df.p_np[100]}\")\n",
    "molecule = molecule_from_smiles(df.iloc[100].smiles)\n",
    "print(\"Molecule:\")\n",
    "molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "graph = graph_from_molecule(molecule)\n",
    "print(\"Graph (including self-loops):\")\n",
    "print(\"\\tatom features\\t\", graph[0].shape)\n",
    "print(\"\\tbond features\\t\", graph[1].shape)\n",
    "print(\"\\tpair indices\\t\", graph[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Create a `tf.data.Dataset`\n",
    "\n",
    "In this tutorial, the MPNN implementation will take as input (per iteration) a single graph.\n",
    "Therefore, given a batch of (sub)graphs (molecules), we need to merge them into a\n",
    "single graph (we'll refer to this graph as *global graph*).\n",
    "This global graph is a disconnected graph where each subgraph is\n",
    "completely separated from the other subgraphs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(x_batch, y_batch):\n",
    "    \"\"\"Merges (sub)graphs of batch into a single global (disconnected) graph\n",
    "    \"\"\"\n",
    "\n",
    "    atom_features, bond_features, pair_indices = x_batch\n",
    "\n",
    "    # Obtain number of atoms and bonds for each graph (molecule)\n",
    "    num_atoms = atom_features.row_lengths()\n",
    "    num_bonds = bond_features.row_lengths()\n",
    "\n",
    "    # Obtain partition indices. atom_partition_indices will be used to\n",
    "    # gather (sub)graphs from global graph in model later on\n",
    "    molecule_indices = tf.range(len(num_atoms))\n",
    "    atom_partition_indices = tf.repeat(molecule_indices, num_atoms)\n",
    "    bond_partition_indices = tf.repeat(molecule_indices[:-1], num_bonds[1:])\n",
    "\n",
    "    # Merge (sub)graphs into a global (disconnected) graph. Adding 'increment' to\n",
    "    # 'pair_indices' (and merging ragged tensors) actualizes the global graph\n",
    "    increment = tf.cumsum(num_atoms[:-1])\n",
    "    increment = tf.pad(\n",
    "        tf.gather(increment, bond_partition_indices), [(num_bonds[0], 0)]\n",
    "    )\n",
    "    pair_indices = pair_indices.merge_dims(outer_axis=0, inner_axis=1).to_tensor()\n",
    "    pair_indices = pair_indices + increment[:, tf.newaxis]\n",
    "    atom_features = atom_features.merge_dims(outer_axis=0, inner_axis=1).to_tensor()\n",
    "    bond_features = bond_features.merge_dims(outer_axis=0, inner_axis=1).to_tensor()\n",
    "\n",
    "    return (atom_features, bond_features, pair_indices, atom_partition_indices), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MPNNDataset(X, y, batch_size=32, shuffle=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, (y)))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1024)\n",
    "    return dataset.batch(batch_size).map(prepare_batch, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Model\n",
    "\n",
    "The MPNN model can take on various shapes and forms. In this tutorial, we will implement an\n",
    "MPNN based on the original paper\n",
    "[Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212) and\n",
    "[DeepChem's MPNNModel](https://deepchem.readthedocs.io/en/latest/api_reference/models.html#mpnnmodel).\n",
    "The MPNN of this tutorial consists of three stages: message passing, readout and\n",
    "classification.\n",
    "\n",
    "\n",
    "### Message passing\n",
    "\n",
    "The message passing step itself consists of two parts:\n",
    "\n",
    "1. The *edge network*, which passes messages from 1-hop neighbors `w^{t}_{i}` of `v^{t}`\n",
    "to `v^{t}`, based on the edge features between them (`e_{v^{t}w^{t}_{i}}`, where `t =\n",
    "0`), resulting in an updated node state `v^{t+1}`. `_{i}` denotes the `i:th` neighbor of\n",
    "`v^{t}` and `^{t}` the `t:th` state of `v` or `w`. An important feature of the edge\n",
    "network (in contrast to e.g. the relational graph convolutional network) is that it\n",
    "allows for non-discrete edge features. However, in this tutorial, only discrete edge\n",
    "features will be used.\n",
    "\n",
    "\n",
    "2. The *gated recurrent unit* (GRU), which takes as input the most recent node state\n",
    "(e.g., `v^{t+1}`) and updates it based on previous node state(s) (e.g., `v^{t}`). In\n",
    "other words, the most recent node states serves as the input to the GRU, while the previous\n",
    "node state(s) are incorporated within the memory state of the GRU.\n",
    "\n",
    "Importantly, step (1) and (2) are repeated for `k steps`, and where at each step `1...k`,\n",
    "the radius (or # hops) of aggregated information from the source node `v` increases by 1.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeNetwork(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.atom_dim = input_shape[0][-1]\n",
    "        self.bond_dim = input_shape[1][-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(self.bond_dim, self.atom_dim * self.atom_dim),\n",
    "            trainable=True,\n",
    "            initializer=\"glorot_uniform\",\n",
    "        )\n",
    "        self.bias = self.add_weight(\n",
    "            shape=(self.atom_dim * self.atom_dim), trainable=True, initializer=\"zeros\",\n",
    "        )\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, bond_features, pair_indices = inputs\n",
    "\n",
    "        # Apply linear transformation to bond features\n",
    "        bond_features = tf.matmul(bond_features, self.kernel) + self.bias\n",
    "\n",
    "        # Reshape for neighborhood aggregation later\n",
    "        bond_features = tf.reshape(bond_features, (-1, self.atom_dim, self.atom_dim))\n",
    "\n",
    "        # Obtain atom features of neighbors\n",
    "        atom_features_neighbors = tf.gather(atom_features, pair_indices[:, 1])\n",
    "        atom_features_neighbors = tf.expand_dims(atom_features_neighbors, axis=-1)\n",
    "\n",
    "        # Apply neighborhood aggregation\n",
    "        transformed_features = tf.matmul(bond_features, atom_features_neighbors)\n",
    "        transformed_features = tf.squeeze(transformed_features, axis=-1)\n",
    "        aggregated_features = tf.math.segment_sum(\n",
    "            transformed_features, pair_indices[:, 0]\n",
    "        )\n",
    "        return aggregated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassing(layers.Layer):\n",
    "    def __init__(self, units, steps=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.steps = steps\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.atom_dim = input_shape[0][-1]\n",
    "        self.message_step = EdgeNetwork()\n",
    "        self.pad_length = max(0, self.units - self.atom_dim)\n",
    "        self.update_step = layers.GRUCell(self.atom_dim + self.pad_length)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, bond_features, pair_indices = inputs\n",
    "\n",
    "        # Pad atom features if number of desired units exceeds atom_features dim\n",
    "        atom_features_updated = tf.pad(atom_features, [(0, 0), (0, self.pad_length)])\n",
    "\n",
    "        # Perform a number of steps of message passing\n",
    "        for i in range(self.steps):\n",
    "            # Aggregate atom_features from neighbors\n",
    "            atom_features_aggregated = self.message_step(\n",
    "                [atom_features_updated, bond_features, pair_indices]\n",
    "            )\n",
    "\n",
    "            # Update aggregated atom_features via a step of GRU\n",
    "            atom_features_updated, _ = self.update_step(\n",
    "                atom_features_aggregated, atom_features_updated\n",
    "            )\n",
    "        return atom_features_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Readout\n",
    "\n",
    "When the message passing procedure ends, the k-step-aggregated node states are to be partitioned\n",
    "into subgraphs (correspoding to each molecule in the batch) and subsequently\n",
    "reduced to graph-level embeddings. In the\n",
    "[original paper](https://arxiv.org/abs/1704.01212), a\n",
    "[set-to-set layer](https://arxiv.org/abs/1511.06391) was used for this purpose.\n",
    "In this tutorial however, a transformer encoder will be used. Specifically:\n",
    "\n",
    "* the k-step-aggregated node states will be partitioned into the subgraphs\n",
    "(corresponding to each molecule in the batch);\n",
    "* each subgraph will then be padded to match the subgraph with the greatest number of nodes, followed\n",
    "by a `tf.stack(...)`;\n",
    "* the (stacked) padded tensor, encoding subgraphs (each subgraph containing sets of node states), are\n",
    "masked to make sure the paddings don't interfere with training;\n",
    "* finally, the padded tensor is passed to the transformer followed by an average pooling.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartitionPadding(layers.Layer):\n",
    "    def __init__(self, batch_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        atom_features, atom_partition_indices = inputs\n",
    "\n",
    "        # Obtain subgraphs\n",
    "        atom_features = tf.dynamic_partition(\n",
    "            atom_features, atom_partition_indices, self.batch_size\n",
    "        )\n",
    "\n",
    "        # Pad and stack subgraphs\n",
    "        num_atoms = [tf.shape(f)[0] for f in atom_features]\n",
    "        max_num_atoms = tf.reduce_max(num_atoms)\n",
    "        atom_features_padded = tf.stack(\n",
    "            [\n",
    "                tf.pad(f, [(0, max_num_atoms - n), (0, 0)])\n",
    "                for f, n in zip(atom_features, num_atoms)\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        # Remove empty subgraphs (usually for last batch)\n",
    "        nonempty_examples = tf.where(tf.reduce_sum(atom_features_padded, (1, 2)) != 0)\n",
    "        nonempty_examples = tf.squeeze(nonempty_examples, axis=-1)\n",
    "\n",
    "        return tf.gather(atom_features_padded, nonempty_examples, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, num_heads=8, embed_dim=64, dense_dim=512, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.attention = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        attention_mask = mask[:, tf.newaxis, :] if mask is not None else None\n",
    "        attention_output = self.attention(inputs, inputs, attention_mask=attention_mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        return self.layernorm_2(proj_input + self.dense_proj(proj_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Message Passing Neural Network (MPNN)\n",
    "\n",
    "It is now time to complete the MPNN model. In addition to the message passing\n",
    "and readout, a two-layer classification network will be implemented to make\n",
    "predictions of BBBP.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MPNNModel(\n",
    "    atom_dim,\n",
    "    bond_dim,\n",
    "    batch_size=32,\n",
    "    message_units=64,\n",
    "    message_steps=4,\n",
    "    num_attention_heads=8,\n",
    "    dense_units=512,\n",
    "):\n",
    "    atom_features = layers.Input((atom_dim), dtype=\"float32\", name=\"atom_features\")\n",
    "    bond_features = layers.Input((bond_dim), dtype=\"float32\", name=\"bond_features\")\n",
    "    pair_indices = layers.Input((2), dtype=\"int32\", name=\"pair_indices\")\n",
    "    atom_partition_indices = layers.Input(\n",
    "        (), dtype=\"int32\", name=\"atom_partition_indices\"\n",
    "    )\n",
    "\n",
    "    x = MessagePassing(message_units, message_steps)(\n",
    "        [atom_features, bond_features, pair_indices]\n",
    "    )\n",
    "\n",
    "    x = PartitionPadding(batch_size)([x, atom_partition_indices])\n",
    "\n",
    "    x = layers.Masking()(x)\n",
    "\n",
    "    x = TransformerEncoder(num_attention_heads, message_units, dense_units)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(dense_units, activation=\"relu\")(x)\n",
    "    x = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[atom_features, bond_features, pair_indices, atom_partition_indices],\n",
    "        outputs=[x],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnn = MPNNModel(\n",
    "    atom_dim=x_train[0][0][0].shape[0], bond_dim=x_train[1][0][0].shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpnn.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
    "    metrics=[keras.metrics.AUC(name=\"AUC\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(mpnn, show_dtype=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Training\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MPNNDataset(x_train, y_train)\n",
    "valid_dataset = MPNNDataset(x_valid, y_valid)\n",
    "test_dataset = MPNNDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = mpnn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=40,\n",
    "    verbose=2,\n",
    "    class_weight={0: 2.0, 1: 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history[\"AUC\"], label=\"train AUC\")\n",
    "plt.plot(history.history[\"val_AUC\"], label=\"valid AUC\")\n",
    "plt.xlabel(\"Epochs\", fontsize=16)\n",
    "plt.ylabel(\"AUC\", fontsize=16)\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### Predicting\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecules = [molecule_from_smiles(df.smiles.values[index]) for index in test_index]\n",
    "y_true = [df.p_np.values[index] for index in test_index]\n",
    "y_pred = tf.squeeze(mpnn.predict(test_dataset), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legends = [f\"y_true/y_pred = {y_true[i]}/{y_pred[i]:.2f}\" for i in range(len(y_true))]\n",
    "MolsToGridImage(molecules, molsPerRow=4, legends=legends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Conclusions\n",
    "\n",
    "In this tutorial, we demonstarted a message passing neural network (MPNN) to\n",
    "predict blood-brain barrier permeability (BBBP) for a number of different molecules. We\n",
    "first had to construct graphs from SMILES, and then build a Keras model that could\n",
    "operate on these graphs.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
